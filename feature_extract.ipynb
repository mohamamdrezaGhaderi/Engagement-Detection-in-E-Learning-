{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\Mohammed\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import dlib\n",
    "from imutils import face_utils\n",
    "from scipy.spatial import distance as dist\n",
    "from scipy.spatial import ConvexHull\n",
    "from deepface import DeepFace\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "detector = dlib.get_frontal_face_detector()\n",
    "predictor = dlib.shape_predictor('shape_predictor_68_face_landmarks.dat')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eye_aspect_ratio(eye):\n",
    "    A = dist.euclidean(eye[1], eye[5])\n",
    "    B = dist.euclidean(eye[2], eye[4])\n",
    "    C = dist.euclidean(eye[0], eye[3])\n",
    "    return (A + B) / (2.0 * C)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mouth_aspect_ratio(mouth):\n",
    "    A = dist.euclidean(mouth[13], mouth[19])\n",
    "    B = dist.euclidean(mouth[15], mouth[17])\n",
    "    C = dist.euclidean(mouth[12], mouth[16])\n",
    "    return (A + B) / (2.0 * C)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_area(landmarks):\n",
    "    hull = ConvexHull(landmarks)\n",
    "    return hull.volume\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eyebrow_distance(left_eyebrow, right_eyebrow):\n",
    "    return dist.euclidean(left_eyebrow[-1], right_eyebrow[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def head_tilt_angle(shape):\n",
    "    nose = shape[33]\n",
    "    chin = shape[8]\n",
    "    return nose[1] - chin[1]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gaze_direction(eye):\n",
    "    left_corner = eye[0]\n",
    "    right_corner = eye[3]\n",
    "    top_center = (eye[1] + eye[2]) / 2\n",
    "    bottom_center = (eye[4] + eye[5]) / 2\n",
    "\n",
    "    horizontal_ratio = (right_corner[0] - left_corner[0]) / (bottom_center[1] - top_center[1])\n",
    "    return horizontal_ratio\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_emotion(face_roi):\n",
    "    try:\n",
    "        result = DeepFace.analyze(face_roi, actions=['emotion'], enforce_detection=False)\n",
    "        dominant_emotion = result['dominant_emotion']\n",
    "        emotion_confidence = result['emotion'][dominant_emotion]\n",
    "        return dominant_emotion, emotion_confidence\n",
    "    except:\n",
    "        return \"Neutral\", 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def extract_features_from_video(video_path, label, fps=30):\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    frame_count = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "    frame_interval = max(1, frame_count // 10)  # Sample 10 frames per video\n",
    "    features = []\n",
    "\n",
    "    for i in range(0, frame_count, frame_interval):\n",
    "        cap.set(cv2.CAP_PROP_POS_FRAMES, i)\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "\n",
    "        gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "        faces = detector(gray)\n",
    "\n",
    "        for face in faces:\n",
    "            shape = predictor(gray, face)\n",
    "            shape = face_utils.shape_to_np(shape)\n",
    "\n",
    "            # Extract landmarks\n",
    "            left_eye = shape[36:42]\n",
    "            right_eye = shape[42:48]\n",
    "            mouth = shape[48:68]\n",
    "            left_eyebrow = shape[17:22]\n",
    "            right_eyebrow = shape[22:27]\n",
    "\n",
    "            # Compute features\n",
    "            ear = (eye_aspect_ratio(left_eye) + eye_aspect_ratio(right_eye)) / 2.0\n",
    "            mar = mouth_aspect_ratio(mouth)\n",
    "            head_tilt = head_tilt_angle(shape)\n",
    "            mouth_area = calculate_area(mouth)\n",
    "            eye_area = calculate_area(left_eye) + calculate_area(right_eye)\n",
    "            eyebrow_dist = eyebrow_distance(left_eyebrow, right_eyebrow)\n",
    "            gaze = gaze_direction(left_eye)\n",
    "            \n",
    "            # Emotion Detection\n",
    "            (x, y, w, h) = face_utils.rect_to_bb(face)\n",
    "            face_roi = frame[y:y + h, x:x + w]\n",
    "            emotion, emotion_conf = extract_emotion(face_roi)\n",
    "\n",
    "            # Append features\n",
    "            features.append([\n",
    "                ear, mar, head_tilt, mouth_area, eye_area, \n",
    "                eyebrow_dist, gaze, emotion, emotion_conf, label\n",
    "            ])\n",
    "    cap.release()\n",
    "    return features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "34\n",
      "Balanced feature extraction complete and saved to 'balanced_extracted_features.csv'.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load dataset\n",
    "dataset = pd.read_csv('DAiSEE/Labels/TrainLabels.csv')\n",
    "dataset['ClipID'] = dataset['ClipID'].astype(str)\n",
    "\n",
    "# Find the minimum number of videos per engagement group\n",
    "min_videos_per_group = dataset['Engagement'].value_counts().min()\n",
    "print(min_videos_per_group)\n",
    "# Sample an equal number of videos from each group\n",
    "balanced_videos = []\n",
    "for engagement_level in dataset['Engagement'].unique():\n",
    "    group_videos = dataset[dataset['Engagement'] == engagement_level]\n",
    "    sampled_videos = group_videos.sample(min_videos_per_group, random_state=42)\n",
    "    balanced_videos.append(sampled_videos)\n",
    "\n",
    "# Concatenate the sampled videos into a single DataFrame\n",
    "balanced_dataset = pd.concat(balanced_videos)\n",
    "\n",
    "# Extract features for the balanced dataset\n",
    "all_features = []\n",
    "for index, row in balanced_dataset.iterrows():\n",
    "    video_name = row['ClipID'].replace('.avi', '')\n",
    "    label = row['Engagement']\n",
    "    folder_1 = video_name[:6]\n",
    "    folder_2 = video_name[:10]\n",
    "    video_path = f\"DAiSEE/DataSet/Train/{folder_1}/{folder_2}/{video_name}.avi\"\n",
    "\n",
    "    try:\n",
    "        # Call the feature extraction function\n",
    "        video_features = extract_features_from_video(video_path, label)\n",
    "        all_features.extend(video_features)\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {video_name}: {e}\")\n",
    "\n",
    "# Save extracted features\n",
    "columns = [\n",
    "    'EAR', 'MAR', 'Head_Tilt', 'Mouth_Area', 'Eye_Area', \n",
    "    'Eyebrow_Dist', 'Gaze', 'Emotion', 'Emotion_Conf', 'Engagement'\n",
    "]\n",
    "features_df = pd.DataFrame(all_features, columns=columns)\n",
    "features_df.to_csv('balanced_extracted_features_v1.csv', index=False)\n",
    "print(\"Balanced feature extraction complete and saved to 'balanced_extracted_features.csv'.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enhanced balanced feature extraction complete and saved to 'balanced_extracted_features_12000.csv'.\n"
     ]
    }
   ],
   "source": [
    "# Load dataset\n",
    "dataset = pd.read_csv('DAiSEE/Labels/TrainLabels.csv')\n",
    "dataset['ClipID'] = dataset['ClipID'].astype(str)\n",
    "\n",
    "# Limit the number of videos per class to 34\n",
    "videos_per_class = 34\n",
    "balanced_videos = []\n",
    "for engagement_level in dataset['Engagement'].unique():\n",
    "    group_videos = dataset[dataset['Engagement'] == engagement_level]\n",
    "    sampled_videos = group_videos.head(videos_per_class)  # Use head() instead of sample() if the dataset is small\n",
    "    balanced_videos.append(sampled_videos)\n",
    "\n",
    "# Concatenate the sampled videos into a single DataFrame\n",
    "balanced_dataset = pd.concat(balanced_videos)\n",
    "\n",
    "# Feature extraction with increased frame sampling\n",
    "all_features = []\n",
    "for index, row in balanced_dataset.iterrows():\n",
    "    video_name = row['ClipID'].replace('.avi', '')\n",
    "    label = row['Engagement']\n",
    "    folder_1 = video_name[:6]\n",
    "    folder_2 = video_name[:10]\n",
    "    video_path = f\"DAiSEE/DataSet/Train/{folder_1}/{folder_2}/{video_name}.avi\"\n",
    "\n",
    "    try:\n",
    "        cap = cv2.VideoCapture(video_path)\n",
    "        frame_count = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "        frame_interval = max(1, frame_count // 88)  # Extract ~88 frames per video\n",
    "        \n",
    "        for i in range(0, frame_count, frame_interval):\n",
    "            cap.set(cv2.CAP_PROP_POS_FRAMES, i)\n",
    "            ret, frame = cap.read()\n",
    "            if not ret:\n",
    "                break\n",
    "            \n",
    "            gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "            faces = detector(gray)\n",
    "\n",
    "            for face in faces:\n",
    "                shape = predictor(gray, face)\n",
    "                shape = face_utils.shape_to_np(shape)\n",
    "\n",
    "                left_eye = shape[36:42]\n",
    "                right_eye = shape[42:48]\n",
    "                mouth = shape[48:68]\n",
    "                left_eyebrow = shape[17:22]\n",
    "                right_eyebrow = shape[22:27]\n",
    "\n",
    "                # Compute features\n",
    "                ear = (eye_aspect_ratio(left_eye) + eye_aspect_ratio(right_eye)) / 2.0\n",
    "                mar = mouth_aspect_ratio(mouth)\n",
    "                head_tilt = shape[33][1] - shape[8][1]\n",
    "                mouth_area = calculate_area(mouth)\n",
    "                eye_area = calculate_area(left_eye) + calculate_area(right_eye)\n",
    "                eyebrow_dist = eyebrow_distance(left_eyebrow, right_eyebrow)\n",
    "\n",
    "                # Append features\n",
    "                all_features.append([ear, mar, head_tilt, mouth_area, eye_area, eyebrow_dist, label])\n",
    "\n",
    "        cap.release()\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {video_name}: {e}\")\n",
    "\n",
    "# Save features\n",
    "columns = ['EAR', 'MAR', 'Head_Tilt', 'Mouth_Area', 'Eye_Area', 'Eyebrow_Dist', 'Engagement']\n",
    "features_df = pd.DataFrame(all_features, columns=columns)\n",
    "features_df.to_csv('balanced_extracted_features_12000.csv', index=False)\n",
    "print(\"Enhanced balanced feature extraction complete and saved to 'balanced_extracted_features_12000.csv'.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
